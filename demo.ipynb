{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "import sys\n",
    "# 绘图\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "tf.enable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(r'images/origin/faces.jpg')\n",
    "image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "\"\"\"\n",
    "image = plt.imread(\"Anthony_Hopkins_0002.jpg\")\n",
    "\"\"\"\n",
    "h, w, _ = image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PNet(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(PNet, self).__init__()\n",
    "        self.conv1 = conv2d(10, 3, 1, 'valid', name='conv1')\n",
    "        self.prelu1 = tf.keras.layers.PReLU(name='prelu1', shared_axes=[1, 2])\n",
    "        self.pool1 = pool2d(name='pool1')\n",
    "\n",
    "        self.conv2 = conv2d(16, 3, 1, 'valid', name='conv2')\n",
    "        self.prelu2 = tf.keras.layers.PReLU(name='prelu2', shared_axes=[1, 2])\n",
    "\n",
    "        self.conv3 = conv2d(32, 3, 1, 'valid', name='conv3')\n",
    "        self.prelu3 = tf.keras.layers.PReLU(name='prelu3', shared_axes=[1, 2])\n",
    "\n",
    "        self.conv4_1 = conv2d(\n",
    "            2, 1, 1, 'same', activation='softmax', name='conv4_1')\n",
    "        self.conv4_2 = conv2d(4, 1, 1, 'same', name='conv4_2')\n",
    "\n",
    "    def call(self, input_):\n",
    "        out = self.conv1(input_)\n",
    "        out = self.prelu1(out)\n",
    "        out = self.pool1(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.prelu2(out)\n",
    "        out = self.conv3(out)\n",
    "        out = self.prelu3(out)\n",
    "\n",
    "        prob = self.conv4_1(out)\n",
    "        loc = self.conv4_2(out)\n",
    "        return prob, loc\n",
    "\n",
    "    def model_variable_initialize(self):\n",
    "        image = tf.random_normal((1, 12, 12, 3))\n",
    "        with tf.name_scope('PNet'):\n",
    "            self.call(image)\n",
    "        print(\"P Completed\")\n",
    "\n",
    "\n",
    "class RNet(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(RNet, self).__init__()\n",
    "        self.conv1 = conv2d(28, 3, 1, 'valid', name='conv1')\n",
    "        self.prelu1 = tf.keras.layers.PReLU(name='prelu1', shared_axes=[1, 2])\n",
    "        self.pool1 = pool2d(3, 2, name='pool1')\n",
    "\n",
    "        self.conv2 = conv2d(48, 3, 1, 'valid', name='conv2')\n",
    "        self.prelu2 = tf.keras.layers.PReLU(name='prelu2', shared_axes=[1, 2])\n",
    "        self.pool2 = pool2d(3, 2, 'valid', name='pool2')\n",
    "\n",
    "        self.conv3 = conv2d(64, 2, 1, 'valid', name='conv3')\n",
    "        self.prelu3 = tf.keras.layers.PReLU(name='prelu3', shared_axes=[1, 2])\n",
    "\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.fc1 = tf.keras.layers.Dense(128, name='fc1')\n",
    "        self.prelu4 = tf.keras.layers.PReLU(name='prelu4')\n",
    "\n",
    "        self.fc2_1 = tf.keras.layers.Dense(\n",
    "            2, activation='softmax', name='fc2_1')\n",
    "        self.fc2_2 = tf.keras.layers.Dense(4, name='fc2_2')\n",
    "\n",
    "    def call(self, input_):\n",
    "        out = self.conv1(input_)\n",
    "        out = self.prelu1(out)\n",
    "        out = self.pool1(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.prelu2(out)\n",
    "        out = self.pool2(out)\n",
    "        out = self.conv3(out)\n",
    "        out = self.prelu3(out)\n",
    "        out = self.flatten(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.prelu4(out)\n",
    "\n",
    "        prob = self.fc2_1(out)\n",
    "        loc = self.fc2_2(out)\n",
    "        return prob, loc\n",
    "\n",
    "    def model_variable_initialize(self):\n",
    "        image = tf.random_normal((1, 24, 24, 3))\n",
    "        with tf.name_scope('RNet'):\n",
    "            self.call(image)\n",
    "        print(\"R Completed\")\n",
    "\n",
    "\n",
    "class ONet(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(ONet, self).__init__()\n",
    "        self.conv1 = conv2d(32, 3, 1, 'valid', name='conv1')\n",
    "        self.prelu1 = tf.keras.layers.PReLU(name='prelu1', shared_axes=[1, 2])\n",
    "        self.pool1 = pool2d(3, 2, name='pool1')\n",
    "\n",
    "        self.conv2 = conv2d(64, 3, 1, 'valid', name='conv2')\n",
    "        self.prelu2 = tf.keras.layers.PReLU(name='prelu2', shared_axes=[1, 2])\n",
    "        self.pool2 = pool2d(3, 2, 'valid', name='pool2')\n",
    "\n",
    "        self.conv3 = conv2d(64, 3, 1, 'valid', name='conv3')\n",
    "        self.prelu3 = tf.keras.layers.PReLU(name='prelu3', shared_axes=[1, 2])\n",
    "        self.pool3 = pool2d(2, 2, name='pool3')\n",
    "\n",
    "        self.conv4 = conv2d(128, 2, 1, 'valid', name='conv4')\n",
    "        self.prelu4 = tf.keras.layers.PReLU(name='prelu4', shared_axes=[1, 2])\n",
    "\n",
    "        self.flatten = tf.keras.layers.Flatten()\n",
    "        self.fc1 = tf.keras.layers.Dense(256, name='fc1')\n",
    "        self.prelu5 = tf.keras.layers.PReLU(name='prelu5')\n",
    "\n",
    "        self.fc2_1 = tf.keras.layers.Dense(\n",
    "            2, activation='softmax', name='fc2_1')\n",
    "        self.fc2_2 = tf.keras.layers.Dense(4, name='fc2_2')\n",
    "        self.fc2_3 = tf.keras.layers.Dense(10, name='fc2_3')\n",
    "\n",
    "    def call(self, input_):\n",
    "        out = self.conv1(input_)\n",
    "        out = self.prelu1(out)\n",
    "        out = self.pool1(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.prelu2(out)\n",
    "        out = self.pool2(out)\n",
    "        out = self.conv3(out)\n",
    "        out = self.prelu3(out)\n",
    "        out = self.pool3(out)\n",
    "        out = self.conv4(out)\n",
    "        out = self.prelu4(out)\n",
    "        out = self.flatten(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.prelu5(out)\n",
    "\n",
    "        prob = self.fc2_1(out)\n",
    "        loc = self.fc2_2(out)\n",
    "        landmark = self.fc2_3(out)\n",
    "        return prob, loc, landmark\n",
    "\n",
    "    def model_variable_initialize(self):\n",
    "        image = tf.random_normal((1, 48, 48, 3))\n",
    "        with tf.name_scope('ONet'):\n",
    "            self.call(image)\n",
    "        print(\"O Completed\")\n",
    "\n",
    "\n",
    "def conv2d(filter, ksize=3, stride=1, padding='same', dilation=1, activation=None, name=\"conv2d\"):\n",
    "    ksize = [ksize] * 2\n",
    "    strides = [stride] * 2\n",
    "    dilation = [dilation] * 2\n",
    "    return tf.keras.layers.Conv2D(filters=filter, kernel_size=ksize, strides=strides, padding=padding, dilation_rate=dilation, activation=activation, name=name)\n",
    "\n",
    "\n",
    "def pool2d(ksize=2, stride=2, padding='same', name='pool2d'):\n",
    "    ksize = [ksize] * 2\n",
    "    strides = [stride]*2\n",
    "    return tf.keras.layers.MaxPool2D(pool_size=ksize, strides=strides, padding=padding, name=name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From D:\\Anaconda\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:642: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "P Completed\n",
      "R Completed\n",
      "O Completed\n"
     ]
    }
   ],
   "source": [
    "P = PNet()\n",
    "R = RNet()\n",
    "O = ONet()\n",
    "P.model_variable_initialize()\n",
    "R.model_variable_initialize()\n",
    "O.model_variable_initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.checkpointable.util.CheckpointLoadStatus at 0x1a58baef940>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P.load_weights(\"./checkpoints/pnet/model\")\n",
    "R.load_weights(\"./checkpoints/rnet/model\")\n",
    "O.load_weights(\"./checkpoints/onet/model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################\n",
    "# Note: first stage for PNet.\n",
    "########################################################################\n",
    "images, scales = generate_samples_from_image(image, 30)\n",
    "total_bboxes = []\n",
    "total_bboxes_ref = []\n",
    "for img, scale in zip(images, scales):\n",
    "    prob, loc = P(np.transpose(img,(0,2,1,3)))\n",
    "    prob = tf.transpose(prob,(0,2,1,3))\n",
    "    loc = tf.transpose(loc,(0,2,1,3))\n",
    "    original_bboxes = generate_original_boxes(loc.numpy(), scale)\n",
    "    filter_mask = tf.argmax(prob, axis=-1)\n",
    "    bboxes_tf = tf.boolean_mask(original_bboxes, filter_mask)\n",
    "    bboxes_ref_tf = tf.boolean_mask(loc, filter_mask)\n",
    "    local_bboxes, local_bboxes_ref = bboxes_nms(\n",
    "        bboxes_tf.numpy(), bboxes_ref_tf.numpy(), 0.5)\n",
    "    total_bboxes.append(local_bboxes)\n",
    "    total_bboxes_ref.append(local_bboxes_ref)\n",
    "total_bboxes = np.concatenate(total_bboxes)\n",
    "total_bboxes_ref = np.concatenate(total_bboxes_ref)\n",
    "bboxes, bboxes_ref = bboxes_nms(total_bboxes, total_bboxes_ref, 0.7)\n",
    "bboxes = bboxes_reg(bboxes, bboxes_ref)\n",
    "bboxes = rec2square(bboxes.copy())\n",
    "bboxes_pnet, pad_bboxes = bboxes_clip(bboxes, w, h)\n",
    "images = crop_image(image, bboxes_pnet, pad_bboxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(56, 24, 24, 3)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnet_prob, rnet_loc = R(np.transpose(images,(0,2,1,3)))\n",
    "scores = rnet_prob[:, 1]\n",
    "filter_mask_rnet = scores > 0.7\n",
    "bboxes_ref_rnet_tf = tf.boolean_mask(rnet_loc, filter_mask_rnet)\n",
    "bboxes_rnet_tf = tf.boolean_mask(bboxes_pnet, filter_mask_rnet)\n",
    "bboxes_rnet = bboxes_select(\n",
    "    bboxes_rnet_tf.numpy(), bboxes_ref_rnet_tf.numpy(), 0.7)\n",
    "bboxes_rnet, pad_bboxes_rnet = bboxes_clip(bboxes_rnet, w, h)\n",
    "images = crop_image(image, bboxes_rnet, pad_bboxes_rnet, [48, 48])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 48, 48, 3)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "onet_prob, onet_loc, onet_landmark = O(np.transpose(images,(0,2,1,3)))\n",
    "scores = onet_prob[:, 1]\n",
    "filter_mask_onet = scores > 0.7\n",
    "bboxes_ref_onet_tf = tf.boolean_mask(onet_loc, filter_mask_onet)\n",
    "bboxes_onet_tf = tf.boolean_mask(bboxes_rnet, filter_mask_onet)\n",
    "\n",
    "bboxes_onet = bboxes_select(\n",
    "    bboxes_onet_tf.numpy(), bboxes_ref_onet_tf.numpy(), 0.7,'min')\n",
    "bboxes_onet, pad_bboxes_onet = bboxes_clip(bboxes_onet, w, h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 23,  78,  48, 102],\n",
       "       [148,  79, 173, 104]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bboxes_onet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9991359"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(scores.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1a58eca2470>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x1,y1,x2,y2 = [156,  42, 189,  75]\n",
    "temp = image[y1:y2,x1:x2]\n",
    "plt.imshow(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.imread(r'‪C:\\Users\\Mashiro\\Desktop\\m.png',cv2.CV_LOAD_IMAGE_UNCHANGED)\n",
    "#plt.imshow(image)\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = temp.copy()\n",
    "a[:,:,0] = temp[:,:,2]\n",
    "a[:,:,2] = temp[:,:,0]\n",
    "cv2.imwrite('face_02.jpg',a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imsave('face_03.jpg',temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Detect(object):\n",
    "    def __init__(self, pnet, rnet, onet):\n",
    "        self.P = pnet\n",
    "        self.R = rnet\n",
    "        self.O = onet\n",
    "\n",
    "    def detect_face(self, image, minsize=30):\n",
    "        h, w, _ = image.shape\n",
    "        images, scales = generate_samples_from_image(image, minsize)\n",
    "        ########################################################################\n",
    "        # Note: first stage for PNet.\n",
    "        ########################################################################\n",
    "        total_bboxes = []\n",
    "        total_bboxes_ref = []\n",
    "        for img, scale in zip(images, scales):\n",
    "            pnet_prob, pnet_loc = self.P(np.transpose(img, (0, 2, 1, 3)))\n",
    "            pnet_prob = tf.transpose(pnet_prob, (0, 2, 1, 3))\n",
    "            pnet_loc = tf.transpose(pnet_loc, (0, 2, 1, 3))\n",
    "            original_bboxes = generate_original_boxes(pnet_loc.numpy(), scale)\n",
    "            filter_mask = tf.argmax(pnet_prob, axis=-1)\n",
    "            bboxes_tf = tf.boolean_mask(original_bboxes, filter_mask)\n",
    "            bboxes_ref_tf = tf.boolean_mask(pnet_loc, filter_mask)\n",
    "            local_bboxes, local_bboxes_ref = bboxes_nms(\n",
    "                bboxes_tf.numpy(), bboxes_ref_tf.numpy(), 0.6)\n",
    "            total_bboxes.append(local_bboxes)\n",
    "            total_bboxes_ref.append(local_bboxes_ref)\n",
    "        total_bboxes = np.concatenate(total_bboxes)\n",
    "        total_bboxes_ref = np.concatenate(total_bboxes_ref)\n",
    "        bboxes_pnet = bboxes_select(total_bboxes, total_bboxes_ref, 0.7)\n",
    "        if bboxes_pnet.shape[0] == 0:\n",
    "            return None\n",
    "        bboxes_pnet, pad_bboxes_pnet = bboxes_clip(bboxes_pnet, w, h)\n",
    "        images = crop_image(image, bboxes_pnet, pad_bboxes_pnet)\n",
    "        ########################################################################\n",
    "        # Note: second stage for RNet.\n",
    "        ########################################################################\n",
    "        rnet_prob, rnet_loc = self.R(np.transpose(images, (0, 2, 1, 3)))\n",
    "        scores = rnet_prob[:, 1]\n",
    "        filter_mask_rnet = scores > 0.7\n",
    "        bboxes_ref_rnet_tf = tf.boolean_mask(rnet_loc, filter_mask_rnet)\n",
    "        bboxes_rnet_tf = tf.boolean_mask(bboxes_pnet, filter_mask_rnet)\n",
    "        bboxes_rnet = bboxes_select(\n",
    "            bboxes_rnet_tf.numpy(), bboxes_ref_rnet_tf.numpy(), 0.7)\n",
    "        if bboxes_rnet.shape[0] == 0:\n",
    "            return None\n",
    "        bboxes_rnet, pad_bboxes_rnet = bboxes_clip(bboxes_rnet, w, h)\n",
    "        images = crop_image(image, bboxes_rnet, pad_bboxes_rnet, [48, 48])\n",
    "        ########################################################################\n",
    "        # Note: third stage for RNet.\n",
    "        ########################################################################\n",
    "        onet_prob, onet_loc, onet_landmark = self.O(\n",
    "            np.transpose(images, (0, 2, 1, 3)))\n",
    "        scores = onet_prob[:, 1]\n",
    "        filter_mask_onet = scores > 0.7\n",
    "        bboxes_ref_onet_tf = tf.boolean_mask(onet_loc, filter_mask_onet)\n",
    "        bboxes_onet_tf = tf.boolean_mask(bboxes_rnet, filter_mask_onet)\n",
    "        bboxes_onet = bboxes_select(\n",
    "            bboxes_onet_tf.numpy(), bboxes_ref_onet_tf.numpy(), 0.7)\n",
    "        if bboxes_onet.shape[0] == 0:\n",
    "            return None\n",
    "        bboxes_onet, pad_bboxes_onet = bboxes_clip(bboxes_onet, w, h)\n",
    "        return bboxes_onet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detect = Detect(P,R,O)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc = detect.detect_face(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in P.variables:\n",
    "    print(v.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_image(image, size=(12, 12)):\n",
    "    return cv2.resize(image, size)\n",
    "\n",
    "\n",
    "def preprocess_image(image):\n",
    "    \"\"\"Preprocess a image\"\"\"\n",
    "    image_cp = np.copy(image).astype(np.float32)\n",
    "    # regularize the image\n",
    "    image_regular = (image_cp-127.5)/128\n",
    "    # expand the batch_size dim\n",
    "    image_expanded = np.expand_dims(image_regular, axis=0)\n",
    "    return image_expanded\n",
    "\n",
    "\n",
    "def cal_scales(minsize, factor=0.709, size=(12, 12)):\n",
    "    minlen = np.min(size)\n",
    "    s = 12.0/minsize\n",
    "    minlen *= (s*factor)\n",
    "    scales = [s]\n",
    "    while (minlen >= 12):\n",
    "        scales.append(s * np.power(factor, len(scales)))\n",
    "        minlen *= factor\n",
    "    return scales\n",
    "\n",
    "\n",
    "def generate_samples_from_image(image, minsize=20, factor=0.709):\n",
    "    height, weight, _ = image.shape\n",
    "    scales = cal_scales(minsize, factor, size=(weight, height))\n",
    "    images = []\n",
    "    for scale in scales:\n",
    "        w, h = int(np.ceil(weight * scale)), int(np.ceil(height * scale))\n",
    "        images.append(preprocess_image(resize_image(image, size=(h, w))))\n",
    "    return images, scales\n",
    "\n",
    "\n",
    "def generate_original_boxes(bbox_reg, scale):\n",
    "    # bboxes shape of (1,h,w,4)\n",
    "    bboxes = np.zeros_like(bbox_reg, dtype=np.float32)\n",
    "    _, h, w, _ = np.shape(bboxes)\n",
    "    y, x = np.mgrid[0:h, 0:w]\n",
    "    bboxes[0, :, :, 0] = x * 2 / scale\n",
    "    bboxes[0, :, :, 1] = y * 2 / scale\n",
    "    bboxes[0, :, :, 2] = (x * 2+12) / scale\n",
    "    bboxes[0, :, :, 3] = (y * 2+12) / scale\n",
    "    return tf.convert_to_tensor(bboxes)\n",
    "\n",
    "\n",
    "def bboxes_iou(bboxes1, bboxes2, method=\"union\"):\n",
    "    \"\"\"Computing iou between bboxes1 and bboxes2.\n",
    "    Note: bboxes1 and bboxes2 can be multi-dimensional, but should broacastable.\n",
    "    \"\"\"\n",
    "    bboxes1 = np.transpose(bboxes1)\n",
    "    bboxes2 = np.transpose(bboxes2)\n",
    "    # Intersection bbox and volume.\n",
    "    int_ymin = np.maximum(bboxes1[0], bboxes2[0])\n",
    "    int_xmin = np.maximum(bboxes1[1], bboxes2[1])\n",
    "    int_ymax = np.minimum(bboxes1[2], bboxes2[2])\n",
    "    int_xmax = np.minimum(bboxes1[3], bboxes2[3])\n",
    "\n",
    "    int_h = np.maximum(int_ymax - int_ymin, 0.)\n",
    "    int_w = np.maximum(int_xmax - int_xmin, 0.)\n",
    "    int_vol = int_h * int_w\n",
    "    # Union volume.\n",
    "    vol1 = (bboxes1[2] - bboxes1[0]) * (bboxes1[3] - bboxes1[1])\n",
    "    vol2 = (bboxes2[2] - bboxes2[0]) * (bboxes2[3] - bboxes2[1])\n",
    "\n",
    "    if method == \"min\":\n",
    "        iou = int_vol / np.minimum(vol1, vol2)\n",
    "    else:\n",
    "        iou = int_vol / (vol1 + vol2 - int_vol)\n",
    "    return iou\n",
    "\n",
    "\n",
    "def bboxes_nms(bboxes, bboxes_ref, nms_threshold=0.5, method=\"union\"):\n",
    "    \"\"\"Apply non-maximum selection to bounding boxes.\n",
    "    \"\"\"\n",
    "    keep_bboxes = np.ones((bboxes.shape[0],), dtype=np.bool)\n",
    "    for i in range(bboxes.shape[0]-1):\n",
    "        if keep_bboxes[i]:\n",
    "            # Computer overlap with bboxes which are following.\n",
    "            overlap = bboxes_iou(bboxes[i], bboxes[(i+1):], method)\n",
    "            # Overlap threshold for keeping + checking part of the same class\n",
    "            keep_overlap = overlap < nms_threshold\n",
    "            keep_bboxes[(i+1):] = np.logical_and(keep_bboxes[(i+1):],\n",
    "                                                 keep_overlap)\n",
    "    idxes = np.where(keep_bboxes)\n",
    "    return bboxes[idxes], bboxes_ref[idxes]\n",
    "\n",
    "\n",
    "def bboxes_reg(bboxes, bboxes_ref):\n",
    "    \"\"\"boxes bounding regression.\n",
    "\n",
    "    Args:\n",
    "        bboxes ([array]): original boxes\n",
    "        bboxes_ref ([array]): [description]\n",
    "\n",
    "    Returns:\n",
    "        [array]: [description]\n",
    "    \"\"\"\n",
    "    bboxes = bboxes.astype(np.float32)\n",
    "    w = bboxes[:, 2] - bboxes[:, 0] + 1\n",
    "    h = bboxes[:, 3] - bboxes[:, 1] + 1\n",
    "    bboxes[:, 0] += bboxes_ref[:, 0] * w\n",
    "    bboxes[:, 1] += bboxes_ref[:, 1] * h\n",
    "    bboxes[:, 2] += bboxes_ref[:, 2] * w\n",
    "    bboxes[:, 3] += bboxes_ref[:, 3] * h\n",
    "    return bboxes\n",
    "\n",
    "\n",
    "def rec2square(bboxes):\n",
    "    \"\"\"Convert bbox to square.\"\"\"\n",
    "    h = bboxes[:, 3]-bboxes[:, 1]\n",
    "    w = bboxes[:, 2]-bboxes[:, 0]\n",
    "    l = np.maximum(w, h)\n",
    "    bboxes[:, 0] = bboxes[:, 0]+w*0.5-l*0.5\n",
    "    bboxes[:, 1] = bboxes[:, 1]+h*0.5-l*0.5\n",
    "    bboxes[:, 2:4] = bboxes[:, 0:2] + np.transpose(np.tile(l, (2, 1)))\n",
    "    return bboxes\n",
    "\n",
    "\n",
    "def bboxes_clip(bboxes, w, h):\n",
    "    \"\"\"Clip bounding boxes\n",
    "\n",
    "    Args:\n",
    "        bboxes ([array]): shape of (N, 4)\n",
    "        w ([int]): the width of input image\n",
    "        h ([int]): the heigh of input image\n",
    "\n",
    "    Returns:\n",
    "        [tuple of array]: return the position of cliped boxes and\n",
    "        padding for each axis\n",
    "    \"\"\"\n",
    "    bboxes_cp = bboxes.copy()\n",
    "    boxes_w = (bboxes[:, 2]-bboxes[:, 0]+1).astype(np.int32)\n",
    "    boxes_h = (bboxes[:, 3] - bboxes[:, 0] + 1).astype(np.int32)\n",
    "    bboxes_cp[:, 0] = np.maximum(bboxes[:, 0], 0)\n",
    "    bboxes_cp[:, 1] = np.maximum(bboxes[:, 1], 0)\n",
    "    bboxes_cp[:, 2] = np.minimum(bboxes[:, 2], w)\n",
    "    bboxes_cp[:, 3] = np.minimum(bboxes[:, 3], h)\n",
    "\n",
    "    pad_bboxes = np.zeros_like(bboxes, dtype=np.int32)\n",
    "    pad_bboxes[:, 0] = bboxes_cp[:, 0] - bboxes[:, 0]\n",
    "    pad_bboxes[:, 1] = bboxes_cp[:, 1] - bboxes[:, 1]\n",
    "    pad_bboxes[:, 2] = bboxes[:, 2] - bboxes_cp[:, 2]\n",
    "    pad_bboxes[:, 3] = bboxes[:, 3] - bboxes_cp[:, 3]\n",
    "    pad_bboxes[pad_bboxes < 0] = 0\n",
    "    return bboxes_cp.astype(np.int32), pad_bboxes\n",
    "\n",
    "\n",
    "def crop_image(image, bboxes, pad_bboxes, size=[24, 24]):\n",
    "    # example shape of [N,24,24,3]\n",
    "    shape = [bboxes.shape[0]] + list(size) + [3]\n",
    "    images = np.zeros(shape, dtype=np.float32)\n",
    "    for idx in range(bboxes.shape[0]):\n",
    "        x1, y1, x2, y2 = bboxes[idx]\n",
    "        padding = ((pad_bboxes[idx, 1], pad_bboxes[idx, 3]),\n",
    "                   (pad_bboxes[idx, 0], pad_bboxes[idx, 2]),\n",
    "                   (0, 0))\n",
    "        temp_img = image[y1:y2, x1:x2, :]\n",
    "        temp_img = np.pad(\n",
    "            temp_img, padding, 'constant', constant_values=0)\n",
    "        images[idx] = resize_image(temp_img, size=tuple(size))\n",
    "    return (images-127.5)/128\n",
    "\n",
    "\n",
    "def bboxes_select(bboxes, bboxes_ref, threshold=0.5, method=\"union\"):\n",
    "    \"\"\"A series operation for boxes selected.\n",
    "\n",
    "    Args:\n",
    "        bboxes ([array]): Original locations for boxes, shape of (N, 4).\n",
    "        bboxes_ref ([array): Regression locations for boxes, shape of (N, 4).\n",
    "        threshold (float, optional): Defaults to 0.5.\n",
    "\n",
    "    Returns:\n",
    "        [array]: selected boxes.\n",
    "    \"\"\"\n",
    "    bboxes, bboxes_ref = bboxes_nms(\n",
    "        bboxes, bboxes_ref, threshold, method=method)\n",
    "    '''\n",
    "    if bboxes.shape[0] == 0:\n",
    "        return bboxes\n",
    "    '''\n",
    "    bboxes = bboxes_reg(bboxes, bboxes_ref)\n",
    "    bboxes = rec2square(bboxes.copy())\n",
    "    return bboxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
